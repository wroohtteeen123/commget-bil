# -*- coding = utf-8 -*-
# @Time : 2021-11-16    â°
# @Author : P.B.A.S     ğŸ¥
# @File : Test_1.py     ğŸ«€
# @Software : PyCharm   ğŸ’¾


import urllib.request
import urllib.parse
import gzip
import os
import ssl
import time
import math
import json
import secrets
import pymysql
ssl._create_default_https_context = ssl._create_unverified_context  # å…¨å±€å–æ¶ˆéªŒè¯ã€‚ï¼ˆå…¶å®æˆ‘ä¹Ÿä¸çŸ¥é“è¿™å¥è¯æ˜¯å¹²å˜›çš„ï¼ˆåæ­£åˆ æ‰äº†å°±ä¸èƒ½ç”¨äº†ï¼ˆæŠ¥é”™æ€ä¹ˆåŠå‘¢
import Test_3_NeedHelp


def get_single_page(page_url):  # ç”¨äºè·å¾—å•ä¸ªç½‘ç»œé¡µé¢çš„å‡½æ•°ã€‚

    block_page = {"code": 0, "message": "0", "ttl": 1, "data": {"replies": []}}
    header_bunker = {
        "Accept-Encoding": "gzip, deflate",
        "User-Agent": "Mozilla/5.0 (Macintosh; Apple silicon Mac OS X 12_1_0) Gecko/20100101 Firefox/94.0"
    }  # ä¼ªè£…æˆæµè§ˆå™¨ï¼Œä¹Ÿå¯ä»¥åŠ ä¸€äº›åˆ«çš„ã€‚

    page_request = urllib.request.Request(url=page_url, headers=header_bunker)  # æŠŠurlåœ°å€å’Œå¤´éƒ¨æ‰“åŒ…ã€‚
    page_data_raw = urllib.request.urlopen(page_request)                    # å¼€ä¸ªç½‘é¡µï¼ŒæŠŠè¿”å›çš„å†…å®¹ä¼ ç»™page_data_rawã€‚
    page_data_mar = page_data_raw.read()                                    # æŠŠç½‘é¡µè¿”å›çš„æ‰€æœ‰æ•°æ®è¯»å‡ºåˆ°page_data_marã€‚

    try:

        page_data_deco = gzip.decompress(page_data_mar).decode("utf-8")     # å°†marçš„æ•°æ®è§£ç æˆutf-8ï¼Œå­˜åˆ°decoã€‚
        return page_data_deco  # å°†ç½‘é¡µè§£ç å¾—åˆ°çš„æ•°æ®è¿”å›ç»™å‡½æ•°ã€‚

    except:

        print("ERR-æœ‰ç‚¹é—®é¢˜ï¼Œå¯èƒ½æ˜¯è¯„è®ºåŒºè¢«å…³é—­äº†ï¼Œæˆ–è€…æ˜¯é™åˆ¶äº†5é¡µçš„è®¿é—®ã€‚")
        return str(block_page)


def save_page_content(page_data_download, file_name):    # æŠŠæ¯ä¸€é¡µçš„æ–‡ä»¶ä¿å­˜ã€‚

    file_save = open(file_name, "w")                     # æ‰“å¼€saveDataTxtæ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰å°±åˆ›å»ºä¸€ä¸ªã€‚
    file_save.write(page_data_download)                  # æŠŠpage_data_downloadå†…å®¹å†™å…¥ã€‚
    file_save.close()                                    # å…³é—­æ–‡ä»¶ã€‚


def data_usability_test(name_local_doc, mode):                                # ç”¨äºæ£€æµ‹è¿™ä¸€é¡µæ–‡ä»¶æ²¡æœ‰è§†é¢‘ã€‚å¦‚æœæœ‰è¯„è®ºè¿”å›çœŸï¼Œå¦‚æœæ²¡æœ‰è¯„è®ºè¿”å›å‡ã€‚

    file_open_for_end = open(name_local_doc, "r")                       # æ‰“å¼€ä¸Šä¸€ä¸ªæ–‡ä»¶ç”¨äºæ£€éªŒã€‚
    file_content_str_for_end = file_open_for_end.read()                 # æŠŠå†…å®¹å†™åˆ°file_content_str_for_endã€‚
    file_open_for_end.close()   # å…³é—­æ‰“å¼€çš„æ–‡ä»¶ã€‚

    try:

        file_content_dict_for_end = json.loads(file_content_str_for_end)    # æŠŠJsonæ–‡ä»¶è½¬æ¢ä¸ºå­—å…¸ã€‚

    except:

        return False

    if mode == "c":     # è¯„è®ºæ£€æµ‹

        if len(file_content_dict_for_end["data"]["replies"]) == 0:          # è·å–åˆ°çš„è¯„è®ºæ•°é‡æ£€æµ‹ã€‚

            os.remove(name_local_doc)  # åˆ é™¤æ–‡ä»¶ã€‚
            return False    # æ²¡æœ‰è¯„è®ºè¿”å›å‡ã€‚

        else:

            return True     # æœ‰è¯„è®ºè¿”å›çœŸã€‚

    if mode == "v":     # è§†é¢‘æ£€æµ‹

        if len(file_content_dict_for_end["data"]["list"]["vlist"]) == 0:  # è·å–åˆ°çš„è¯„è®ºæ•°é‡æ£€æµ‹ã€‚

            os.remove(name_local_doc)  # åˆ é™¤æ–‡ä»¶ã€‚
            return False  # æ²¡æœ‰è¯„è®ºè¿”å›å‡ã€‚

        else:

            return True  # æœ‰è¯„è®ºè¿”å›çœŸã€‚

    if mode == "f":     # è§†é¢‘æ£€æµ‹

        if len(file_content_dict_for_end["data"]["list"]) == 0:  # è·å–åˆ°çš„è¯„è®ºæ•°é‡æ£€æµ‹ã€‚

            os.remove(name_local_doc)  # åˆ é™¤æ–‡ä»¶ã€‚
            return False  # æ²¡æœ‰è¯„è®ºè¿”å›å‡ã€‚

        else:

            return True  # æœ‰è¯„è®ºè¿”å›çœŸã€‚


def get_full_follow(uid_upper):  # è¿™ä¸ªå‡½æ•°ï¼Œ æ£€æµ‹è¿™ä¸ªç”¨æˆ·å…³æ³¨çš„æ‰€æœ‰ç”¨æˆ·ã€‚

    page_tag = 1
    break_tag = 0

    while True:

        url = "https://api.bilibili.com/x/relation/followings?vmid=%d&pn=%d" % (uid_upper, page_tag)
        data_download = get_single_page(url)  # ä½¿ç”¨å‡½æ•°è·å¾—é¡µçš„å†…å®¹ï¼Œå†ç»™åˆ°data_downloadã€‚
        name_local_doc = "o-saveData_followUid-%d_Page-%d.json" % (uid_upper, page_tag)  # è¿™æ˜¯ä¿å­˜åœ¨æœ¬åœ°çš„ç½‘é¡µæ–‡ä»¶çš„åå­—æˆ–è€…æ˜¯ä½ç½®ã€‚
        save_page_content(data_download, name_local_doc)  # ä½¿ç”¨å‡½æ•°ï¼Œä¿å­˜é¡µçš„å†…å®¹ã€‚
        print("Following: ", page_tag)  # æ‰“å°é¡µé¢å·ç ã€‚

        if not data_usability_test(name_local_doc, "f"):  # è°ƒç”¨æ£€æµ‹æ¯ä¸€é¡µæ˜¯å¦æœ‰è¯„è®ºçš„å‡½æ•°ï¼Œå†³å®šæ˜¯è·³è¿‡æˆ–æ˜¯ä¸­æ–­ã€‚

            print("BRE-ç°åœ¨åº”è¯¥æ˜¯å®Œå…¨ç»“æŸäº†ï¼Œåªèƒ½è®¿é—®å‰250ä¸ªå…³æ³¨ï¼Œä¹Ÿå¯èƒ½ä¸æ˜¯è¿™æ ·ï¼Œæˆ‘å»ºè®®ä½ æ£€æŸ¥ä¸€ä¸‹ï¼Œå¥½å§ï¼Œå†è§ã€‚")
            break

        else:

            # print("Nice")
            # break

            file_open = open(name_local_doc, "r")  # æ‰“å¼€æœ¬åœ°ä¿å­˜çš„æ–‡ä»¶ã€‚
            file_content_str = file_open.read()  # æŠŠå†…å®¹å†™åˆ°file_content_strã€‚
            file_open.close()  # å…³é—­æ–‡ä»¶ã€‚
            file_content_dict = json.loads(file_content_str)  # æŠŠJsonæ–‡ä»¶è½¬æ¢ä¸ºå­—å…¸ã€‚

            if len(file_content_dict["data"]["list"]) < 50:

                break_tag = 1

            for user_temp_id in range(len(file_content_dict["data"]["list"])):  # æ£€æµ‹æœ‰Nä¸ªå›å¤ï¼Œå¾ªç¯Næ¬¡ã€‚

                data_mid = file_content_dict["data"]["list"][user_temp_id]["mid"]
                data_uname = file_content_dict["data"]["list"][user_temp_id]["uname"]
                data_sign = file_content_dict["data"]["list"][user_temp_id]["sign"]

                print(data_mid)
                print(data_uname)
                print(data_sign)

                print("-"*40)

                # get_full_pages(data_av)
                # get_full_video(data_mid)

            if break_tag == 1:

                print("BRE-ç°åœ¨åº”è¯¥æ˜¯å®Œå…¨ç»“æŸäº†ï¼Œæˆ‘çŒœæ˜¯è¿™æ ·ï¼Œä¹Ÿå¯èƒ½ä¸æ˜¯è¿™æ ·ï¼Œæˆ‘å»ºè®®ä½ æ£€æŸ¥ä¸€ä¸‹ï¼Œå¥½å§ï¼Œæ‹œæ‹œã€‚")
                break

        page_tag += 1  # ä¸‹ä¸€ä¸ªé¡µé¢ã€‚
        time.sleep(0.5 + (secrets.randbelow(40000) / 80000))    # ç”Ÿæˆéšæœº0.50-1.00ç§’ä»¥å†…çš„æ•°å­—ã€‚ã€‚

# get_full_follow(289867)
Test_3_NeedHelp.need_help()